{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOss5GgYu0Wqc/zwGMNxqJi"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UE95iEIgVS6q"
      },
      "outputs": [],
      "source": [
        "#pip install langchain_core langchain_anthropic\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jInFgPaLWuR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = \".....\"\n"
      ],
      "metadata": {
        "id": "yIYTl9YjWsRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_anthropic import ChatAnthropic\n",
        "chat_model = ChatAnthropic(model=\"claude-3-opus-20240229\", temperature=0)"
      ],
      "metadata": {
        "id": "TXboNkMZWzR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model.invoke(\"Tell me joke on life\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCG2pzKWW_hm",
        "outputId": "4bd8481a-e9b6-41fd-f45c-68463ce94766"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Sure, here\\'s a joke about life:\\n\\nLife is like a box of chocolates. It doesn\\'t last long if you\\'re fat.\\n\\nThis joke is a play on the famous quote from the movie Forrest Gump, where the character says, \"Life is like a box of chocolates. You never know what you\\'re gonna get.\" The joke takes a humorous twist by suggesting that if you\\'re overweight, you\\'ll eat the chocolates quickly, and the box (representing life) won\\'t last long.\\n\\nPlease note that this joke might be considered insensitive to some people, as it touches on the topic of weight. It\\'s always important to be mindful of your audience when sharing jokes.', additional_kwargs={}, response_metadata={'id': 'msg_01Nb8xCgUmhFBktM5KCyMX6s', 'model': 'claude-3-opus-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 12, 'output_tokens': 152, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-opus-20240229'}, id='run--7b2f53e3-63a1-4915-9d55-310f63a766db-0', usage_metadata={'input_tokens': 12, 'output_tokens': 152, 'total_tokens': 164, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "chat_model.invoke([HumanMessage(\"Tell me joke on life\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBlTtDXSXuOW",
        "outputId": "65d9a289-a1cb-401e-bc84-2989227c5a19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Sure, here\\'s a joke about life:\\n\\nLife is like a box of chocolates. It doesn\\'t last long if you\\'re fat.\\n\\nThis joke is a play on the famous quote from the movie Forrest Gump, where the character says, \"Life is like a box of chocolates. You never know what you\\'re gonna get.\" The joke takes a humorous twist by suggesting that if you\\'re overweight, you\\'ll eat the chocolates quickly, and the box (representing life) won\\'t last long.\\n\\nPlease note that this joke might be considered insensitive to some people, as it touches on the subject of weight. It\\'s always important to be mindful of your audience when sharing jokes.', additional_kwargs={}, response_metadata={'id': 'msg_012jdmUpwWDranx6FfY7qiTE', 'model': 'claude-3-opus-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 12, 'output_tokens': 152, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-opus-20240229'}, id='run--73d4ac2a-9d20-4340-8c4b-075c57ce5d95-0', usage_metadata={'input_tokens': 12, 'output_tokens': 152, 'total_tokens': 164, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "joke_prompt = ChatPromptTemplate.from_messages([('system','you are world class comedian'),('human','tell me joke on {topic}')])"
      ],
      "metadata": {
        "id": "DFyDLwukYvY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joke_prompt.invoke({'topic':'cat'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOd94dzXZSoe",
        "outputId": "ebce3e32-b5fd-49f5-815e-0728108880ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='you are world class comedian', additional_kwargs={}, response_metadata={}), HumanMessage(content='tell me joke on cat', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = joke_prompt | chat_model\n"
      ],
      "metadata": {
        "id": "E-C5p8tZaC2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"topic\": \"cat\"})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-2dXHLuabRo",
        "outputId": "cafa0f7b-db36-41b7-c4ac-43083a36fba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Sure, here's a joke about a cat:\\n\\nWhy did the cat wear a fancy dress?\\n\\nTo go to the fancy feast!\", additional_kwargs={}, response_metadata={'id': 'msg_01FqQLdj7xuh5Rvecyr9wWfa', 'model': 'claude-3-opus-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 17, 'output_tokens': 31, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-opus-20240229'}, id='run--c8a3576b-ebb2-4609-ad65-d68ab1120d27-0', usage_metadata={'input_tokens': 17, 'output_tokens': 31, 'total_tokens': 48, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "str_chain = chain | StrOutputParser()"
      ],
      "metadata": {
        "id": "xp_wRWNRaorc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "str_chain.invoke({'topic':'cat'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "gSJ1IhlCazDx",
        "outputId": "3cb13541-36d5-45f2-8003-fbc8914a3e30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Sure, here's a joke about a cat:\\n\\nWhy did the cat wear a fancy dress?\\n\\nTo go to the fancy feast!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in str_chain.stream({'topic':'cat'}):\n",
        "    print(chunk,end='|')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qubyDRrAbH04",
        "outputId": "8b68add9-277d-433f-8f52-1665628f4d85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|Sure|, here's a| joke about a cat|:\n",
            "\n",
            "Why di|d the cat wear| a fancy| dress|?|\n",
            "\n",
            "To| go| to the fancy| feast|!||"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model = ChatAnthropic(model=\"claude-3-opus-20240229\", temperature=0)\n",
        "chat_model.invoke(\"what is the current date\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PodUiH1sdyWu",
        "outputId": "17838e60-c9ed-4a0f-878d-e44355b526a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"I apologize, but I do not have access to real-time information about the current date. As an AI language model, I don't have an internal clock or calendar. My knowledge is based on the data I was trained on, which has a cut-off date in 2021. For the most accurate and up-to-date information about the current date, please refer to a reliable calendar or your device's date and time settings.\", additional_kwargs={}, response_metadata={'id': 'msg_01XfsugaGYYKft1SWNEnEwEz', 'model': 'claude-3-opus-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 12, 'output_tokens': 94, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-opus-20240229'}, id='run--3a998b47-2eb7-4ff3-be0f-debeb93a72d7-0', usage_metadata={'input_tokens': 12, 'output_tokens': 94, 'total_tokens': 106, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import date\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([(\"system\", 'You know that the current date is \"{current_date}\".'),\n",
        "    (\"human\", \"{question}\")])\n",
        "chain = prompt | chat_model | StrOutputParser()\n",
        "chain.invoke({\n",
        "    \"question\": \"What is the current date?\",\n",
        "    \"current_date\": date.today()\n",
        "})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "2wr1SFvwhamm",
        "outputId": "e1019cbf-3989-4178-a7d8-ca706d0184c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The current date is 2025-06-16.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model.invoke(\n",
        "    \"What was the Old Ship Saloon's total revenue in Q1 2023?\"\n",
        " )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiFVCjdBholG",
        "outputId": "8c59ad0e-77f3-44c2-923c-13ae9b4e5348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"I apologize, but I do not have access to the Old Ship Saloon's financial records or revenue data for Q1 2023. As an AI language model, I do not have real-time information about specific businesses unless it has been made publicly available through reliable sources. If you need this information, I recommend contacting the Old Ship Saloon directly or checking their public financial reports if they are a publicly-traded company.\", additional_kwargs={}, response_metadata={'id': 'msg_01WbqKfCVaiUczwB6UVpJFFF', 'model': 'claude-3-opus-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 25, 'output_tokens': 93, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-opus-20240229'}, id='run--67ee449e-c9d6-4a67-9714-a900e0648087-0', usage_metadata={'input_tokens': 25, 'output_tokens': 93, 'total_tokens': 118, 'input_token_details': {'cache_read': 0, 'cache_creation': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SOURCE = \"\"\"\n",
        "Old Ship Saloon 2023 quarterly revenue numbers:\n",
        "Q1: $174782.38\n",
        "Q2: $467372.38\n",
        "Q3: $474773.38\n",
        "Q4: $389289.23\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", 'You are a helpful assistant. Use the following context when responding:\\n\\n{context}.'),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n",
        "\n",
        "rag_chain = rag_prompt | chat_model | StrOutputParser()\n",
        "\n",
        "rag_chain.invoke({\n",
        "    \"question\": \"What was the Old Ship Saloon's total revenue in Q1 2023?\",\n",
        "    \"context\": SOURCE\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "hf-rbBqSiWtM",
        "outputId": "c2ef1f44-5d2f-4409-ee02-1f8b98f3496d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"According to the provided context, the Old Ship Saloon's total revenue in Q1 2023 was $174,782.38.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.globals import set_debug\n",
        "\n",
        "set_debug(True)\n",
        "\n",
        "from datetime import date\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", 'You know that the current date is \"{current_date}\".'),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n",
        "\n",
        "chain = prompt | chat_model | StrOutputParser()\n",
        "\n",
        "chain.invoke({\n",
        "    \"question\": \"What is the current date?\",\n",
        "    \"current_date\": date.today()\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gjTVMaGxjZb4",
        "outputId": "59e283b1-c477-4b45-d5c5-02481bc4456d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
            "\u001b[0m[outputs]\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatAnthropic] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"System: You know that the current date is \\\"2025-06-16\\\".\\nHuman: What is the current date?\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatAnthropic] [1.92s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"The current date is 2025-06-16.\",\n",
            "        \"generation_info\": null,\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"The current date is 2025-06-16.\",\n",
            "            \"response_metadata\": {\n",
            "              \"id\": \"msg_014cdWy3YZBNw265bkuH6Yvu\",\n",
            "              \"model\": \"claude-3-opus-20240229\",\n",
            "              \"stop_reason\": \"end_turn\",\n",
            "              \"stop_sequence\": null,\n",
            "              \"usage\": {\n",
            "                \"cache_creation_input_tokens\": 0,\n",
            "                \"cache_read_input_tokens\": 0,\n",
            "                \"input_tokens\": 28,\n",
            "                \"output_tokens\": 15,\n",
            "                \"server_tool_use\": null,\n",
            "                \"service_tier\": \"standard\"\n",
            "              },\n",
            "              \"model_name\": \"claude-3-opus-20240229\"\n",
            "            },\n",
            "            \"type\": \"ai\",\n",
            "            \"id\": \"run--706bb8e4-2406-4eb6-b932-d2f26567d70d-0\",\n",
            "            \"usage_metadata\": {\n",
            "              \"input_tokens\": 28,\n",
            "              \"output_tokens\": 15,\n",
            "              \"total_tokens\": 43,\n",
            "              \"input_token_details\": {\n",
            "                \"cache_read\": 0,\n",
            "                \"cache_creation\": 0\n",
            "              }\n",
            "            },\n",
            "            \"tool_calls\": [],\n",
            "            \"invalid_tool_calls\": []\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"id\": \"msg_014cdWy3YZBNw265bkuH6Yvu\",\n",
            "    \"model\": \"claude-3-opus-20240229\",\n",
            "    \"stop_reason\": \"end_turn\",\n",
            "    \"stop_sequence\": null,\n",
            "    \"usage\": {\n",
            "      \"cache_creation_input_tokens\": 0,\n",
            "      \"cache_read_input_tokens\": 0,\n",
            "      \"input_tokens\": 28,\n",
            "      \"output_tokens\": 15,\n",
            "      \"server_tool_use\": null,\n",
            "      \"service_tier\": \"standard\"\n",
            "    },\n",
            "    \"model_name\": \"claude-3-opus-20240229\"\n",
            "  },\n",
            "  \"run\": null,\n",
            "  \"type\": \"LLMResult\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] Entering Parser run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"The current date is 2025-06-16.\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [1.92s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"The current date is 2025-06-16.\"\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The current date is 2025-06-16.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set_debug(False)\n",
        "\n",
        "stream = chain.astream_events({\n",
        "    \"question\": \"What is the current date?\",\n",
        "    \"current_date\": date.today()\n",
        "}, version=\"v1\")\n",
        "\n",
        "async for event in stream:\n",
        "    print(event)\n",
        "    print(\"-----\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfRf_fdPjduY",
        "outputId": "6742a4fb-6951-41d4-c5b5-4911e868ad04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'event': 'on_chain_start', 'run_id': '67aa4f58-8b7c-4119-bcd7-3b754ddbf661', 'name': 'RunnableSequence', 'tags': [], 'metadata': {}, 'data': {'input': {'question': 'What is the current date?', 'current_date': datetime.date(2025, 6, 16)}}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_prompt_start', 'name': 'ChatPromptTemplate', 'run_id': '6dc6644b-f716-4ab9-b196-eeea0e08f689', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'input': {'question': 'What is the current date?', 'current_date': datetime.date(2025, 6, 16)}}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_prompt_end', 'name': 'ChatPromptTemplate', 'run_id': '6dc6644b-f716-4ab9-b196-eeea0e08f689', 'tags': ['seq:step:1'], 'metadata': {}, 'data': {'input': {'question': 'What is the current date?', 'current_date': datetime.date(2025, 6, 16)}, 'output': ChatPromptValue(messages=[SystemMessage(content='You know that the current date is \"2025-06-16\".', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the current date?', additional_kwargs={}, response_metadata={})])}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_chat_model_start', 'name': 'ChatAnthropic', 'run_id': 'e65faf82-0331-47eb-b493-dc30257b069e', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-opus-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'data': {'input': {'messages': [[SystemMessage(content='You know that the current date is \"2025-06-16\".', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the current date?', additional_kwargs={}, response_metadata={})]]}}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': 'e65faf82-0331-47eb-b493-dc30257b069e', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-opus-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_name': 'claude-3-opus-20240229'}, id='run--e65faf82-0331-47eb-b493-dc30257b069e', usage_metadata={'input_tokens': 28, 'output_tokens': 0, 'total_tokens': 28, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_parser_start', 'name': 'StrOutputParser', 'run_id': 'b24895d2-64be-4e6b-8ed6-0d316810fb0c', 'tags': ['seq:step:3'], 'metadata': {}, 'data': {}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_parser_stream', 'name': 'StrOutputParser', 'run_id': 'b24895d2-64be-4e6b-8ed6-0d316810fb0c', 'tags': ['seq:step:3'], 'metadata': {}, 'data': {'chunk': ''}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_chain_stream', 'run_id': '67aa4f58-8b7c-4119-bcd7-3b754ddbf661', 'tags': [], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': ''}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': 'e65faf82-0331-47eb-b493-dc30257b069e', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-opus-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'data': {'chunk': AIMessageChunk(content='The current date is', additional_kwargs={}, response_metadata={}, id='run--e65faf82-0331-47eb-b493-dc30257b069e')}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_parser_stream', 'name': 'StrOutputParser', 'run_id': 'b24895d2-64be-4e6b-8ed6-0d316810fb0c', 'tags': ['seq:step:3'], 'metadata': {}, 'data': {'chunk': 'The current date is'}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_chain_stream', 'run_id': '67aa4f58-8b7c-4119-bcd7-3b754ddbf661', 'tags': [], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': 'The current date is'}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': 'e65faf82-0331-47eb-b493-dc30257b069e', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-opus-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'data': {'chunk': AIMessageChunk(content=' 2025-06', additional_kwargs={}, response_metadata={}, id='run--e65faf82-0331-47eb-b493-dc30257b069e')}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_parser_stream', 'name': 'StrOutputParser', 'run_id': 'b24895d2-64be-4e6b-8ed6-0d316810fb0c', 'tags': ['seq:step:3'], 'metadata': {}, 'data': {'chunk': ' 2025-06'}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_chain_stream', 'run_id': '67aa4f58-8b7c-4119-bcd7-3b754ddbf661', 'tags': [], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': ' 2025-06'}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': 'e65faf82-0331-47eb-b493-dc30257b069e', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-opus-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'data': {'chunk': AIMessageChunk(content='-16.', additional_kwargs={}, response_metadata={}, id='run--e65faf82-0331-47eb-b493-dc30257b069e')}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_parser_stream', 'name': 'StrOutputParser', 'run_id': 'b24895d2-64be-4e6b-8ed6-0d316810fb0c', 'tags': ['seq:step:3'], 'metadata': {}, 'data': {'chunk': '-16.'}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_chain_stream', 'run_id': '67aa4f58-8b7c-4119-bcd7-3b754ddbf661', 'tags': [], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': '-16.'}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_chat_model_stream', 'name': 'ChatAnthropic', 'run_id': 'e65faf82-0331-47eb-b493-dc30257b069e', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-opus-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'stop_reason': 'end_turn', 'stop_sequence': None}, id='run--e65faf82-0331-47eb-b493-dc30257b069e', usage_metadata={'input_tokens': 0, 'output_tokens': 15, 'total_tokens': 15})}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_parser_stream', 'name': 'StrOutputParser', 'run_id': 'b24895d2-64be-4e6b-8ed6-0d316810fb0c', 'tags': ['seq:step:3'], 'metadata': {}, 'data': {'chunk': ''}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_chain_stream', 'run_id': '67aa4f58-8b7c-4119-bcd7-3b754ddbf661', 'tags': [], 'metadata': {}, 'name': 'RunnableSequence', 'data': {'chunk': ''}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_chat_model_end', 'name': 'ChatAnthropic', 'run_id': 'e65faf82-0331-47eb-b493-dc30257b069e', 'tags': ['seq:step:2'], 'metadata': {'ls_provider': 'anthropic', 'ls_model_name': 'claude-3-opus-20240229', 'ls_model_type': 'chat', 'ls_temperature': 0.0, 'ls_max_tokens': 1024}, 'data': {'input': {'messages': [[SystemMessage(content='You know that the current date is \"2025-06-16\".', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is the current date?', additional_kwargs={}, response_metadata={})]]}, 'output': {'generations': [[{'text': 'The current date is 2025-06-16.', 'generation_info': None, 'type': 'ChatGenerationChunk', 'message': AIMessageChunk(content='The current date is 2025-06-16.', additional_kwargs={}, response_metadata={'model_name': 'claude-3-opus-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None}, id='run--e65faf82-0331-47eb-b493-dc30257b069e', usage_metadata={'input_tokens': 28, 'output_tokens': 15, 'total_tokens': 43, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}})}]], 'llm_output': None, 'run': None, 'type': 'LLMResult'}}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_parser_end', 'name': 'StrOutputParser', 'run_id': 'b24895d2-64be-4e6b-8ed6-0d316810fb0c', 'tags': ['seq:step:3'], 'metadata': {}, 'data': {'input': AIMessageChunk(content='The current date is 2025-06-16.', additional_kwargs={}, response_metadata={'model_name': 'claude-3-opus-20240229', 'stop_reason': 'end_turn', 'stop_sequence': None}, id='run--e65faf82-0331-47eb-b493-dc30257b069e', usage_metadata={'input_tokens': 28, 'output_tokens': 15, 'total_tokens': 43, 'input_token_details': {'cache_creation': 0, 'cache_read': 0}}), 'output': 'The current date is 2025-06-16.'}, 'parent_ids': []}\n",
            "-----\n",
            "{'event': 'on_chain_end', 'name': 'RunnableSequence', 'run_id': '67aa4f58-8b7c-4119-bcd7-3b754ddbf661', 'tags': [], 'metadata': {}, 'data': {'output': 'The current date is 2025-06-16.'}, 'parent_ids': []}\n",
            "-----\n"
          ]
        }
      ]
    }
  ]
}